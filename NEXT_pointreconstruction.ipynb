{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point reconstruction in NEXT-NEW\n",
    "\n",
    "In this notebook we read in the prepared data, construct and train the DNN, and then evaluate its performance for reconstruction of point-like events in NEXT-NEW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  as np\n",
    "import random as rd\n",
    "import tables as tb\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib.patches         import Ellipse\n",
    "from __future__  import print_function\n",
    "from scipy.stats import threshold\n",
    "\n",
    "# Keras imports\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.models               import Model, load_model, Sequential\n",
    "from keras.layers               import Input, Dense, MaxPooling3D, AveragePooling3D, Convolution3D, Activation, Dropout, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers           import SGD, Adam, Nadam         \n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core          import Flatten\n",
    "from keras                      import callbacks\n",
    "from keras.regularizers         import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable definitions\n",
    "Here we define key variables to be used throughout the notebook.  Note that we will read the data from a directory `data_location/run_name`, and it is stored in multiple files:\n",
    "- The training data will consist of the events stored in files from `train_fstart` to `train_fend`\n",
    "- The test data will consist of the events stored in files from `test_fstart` to `test_fend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "xdim = 48\n",
    "ydim = 48\n",
    "zdim = 1\n",
    "\n",
    "# data location and training/test file numbers\n",
    "data_fname = \"/Users/jrenner/IFIC/jerenner/next-deeplearning-analysis/data/dnn_kr_100k.h5\"\n",
    "\n",
    "# scale and shift factors: y* = y/fscale + fshift, where y* is label value for net training\n",
    "fscale = 400.\n",
    "fshift = 0.5\n",
    "\n",
    "# other parameters\n",
    "ntrain = 99000 # number of events to use for training and validation\n",
    "sipm_th = 0.01 # threshold for inclusion of SiPM in barycenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "### Data input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the function to read the data from multiple files\n",
    "def read_data(dat_file, ntrain):\n",
    "    \n",
    "    # get the maps and the labels from the data file\n",
    "    indata = tb.open_file(dat_file, 'r')\n",
    "    sum_maps = np.reshape(indata.root.maps,(len(indata.root.maps), 48, 48))\n",
    "    labels = np.array(indata.root.coords,dtype=np.float32)\n",
    "    indata.close()\n",
    "\n",
    "    # reshape the maps to add the extra channel dimension and the labels to fit in the interval [0,1]\n",
    "    x_ = np.reshape(sum_maps[0:ntrain], (len(sum_maps[0:ntrain]), 48, 48, 1))\n",
    "    y_ = labels[0:ntrain,:2]/fscale + fshift\n",
    "    \n",
    "    x_test = np.reshape(sum_maps[ntrain:], (len(sum_maps[ntrain:]), 48, 48, 1))\n",
    "    y_test = labels[ntrain:,:2]/fscale + fshift\n",
    "    \n",
    "    print(\"Finished reading data: {0} training and {1} test events\".format(len(x_),len(x_test)))\n",
    "    return x_,y_,x_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network models\n",
    "These functions should define and return a Keras model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fully-connected neural network with 64 hidden neurons and 1 readout neuron\n",
    "def model_FC(inputs):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(48,48,1)))\n",
    "    model.add(Dense(units=1024, activation='relu')) \n",
    "    model.add(Dense(units=512,  activation='relu'))\n",
    "    model.add(Dense(units=256,  activation='relu'))\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "    model.add(Dense(units=2,    activation='relu'))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=Nadam(lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                                  epsilon=1e-08, schedule_decay=0.01),\n",
    "                                  metrics=['accuracy']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot a 48x48 SiPM map\n",
    "def NEW_SiPM_map_plot(xarr, yarr, normalize=True, zoom=False):\n",
    "    if normalize:\n",
    "        probs = (xarr - np.min(xarr))\n",
    "        probs /= np.max(probs)\n",
    "    else: \n",
    "        probs = xarr\n",
    "\n",
    "    # set up the figure\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    fig.set_figheight(5.0)\n",
    "    fig.set_figwidth(5.0)\n",
    "    ax1.axis([-250, 250, -250, 250])\n",
    "\n",
    "    # draw the SiPMs with the appropriate shading\n",
    "    for i in range(48):\n",
    "        for j in range(48):\n",
    "            r = Ellipse(xy=(i * 10 - 235, j * 10 - 235), width=2., height=2.)\n",
    "            r.set_facecolor('0')\n",
    "            r.set_alpha(probs[i, j] + 0.01)\n",
    "            ax1.add_artist(r)\n",
    "            \n",
    "    # place a large blue circle for actual EL point\n",
    "    xpt = fscale*(yarr[0] - fshift)\n",
    "    ypt = fscale*(yarr[1] - fshift)\n",
    "    mrk = Ellipse(xy=(xpt,ypt), width=4., height=4.)\n",
    "    mrk.set_facecolor('b')\n",
    "    ax1.add_artist(mrk)\n",
    "\n",
    "    # zoom the plot around the most energetic SiPM if enabled\n",
    "    if(zoom):\n",
    "        amax = np.argmax(probs)\n",
    "        imax = int(amax / 48); xmax = imax * 10 - 235\n",
    "        jmax = amax % 48;      ymax = jmax * 10 - 235\n",
    "        ax1.axis([xmax-50,xmax+50,ymax-50,ymax+50])\n",
    "        \n",
    "    plt.xlabel(\"x (mm)\")\n",
    "    plt.ylabel(\"y (mm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a prediction of the (x,y) location based on a charge-weighted average of the SiPM maps\n",
    "def predict_barycenter(x_evts):\n",
    "    \n",
    "    y_pred = []\n",
    "    for emap in x_evts:\n",
    "        \n",
    "        xavg = 0; yavg = 0\n",
    "        qsum = 0\n",
    "        for i in range(48):\n",
    "            for j in range(48):\n",
    "\n",
    "                x = i * 10 - 235\n",
    "                y = j * 10 - 235\n",
    "                q = emap[i][j][0]\n",
    "\n",
    "                if(q > sipm_th):\n",
    "                    xavg += x*q\n",
    "                    yavg += y*q\n",
    "                    qsum += q\n",
    "        \n",
    "        xavg /= qsum\n",
    "        yavg /= qsum\n",
    "        y_pred.append(np.array([xavg,yavg]))\n",
    "        \n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "x_train, y_train, x_test, y_test = read_data(data_fname, ntrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one event with corresponding true point\n",
    "plt_evt = 11\n",
    "NEW_SiPM_map_plot(x_train[plt_evt,:,:,0],y_train[plt_evt],normalize=True,zoom=False)\n",
    "NEW_SiPM_map_plot(x_train[plt_evt,:,:,0],y_train[plt_evt],normalize=True,zoom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set load_model to true and specify the file to load in a previously defined/trained model\n",
    "load_mdl = True\n",
    "mfile = 'models/pt_classifier.h5'\n",
    "\n",
    "if(load_mdl):\n",
    "    model = load_model(mfile)\n",
    "else:\n",
    "\n",
    "    # otherwise define the model\n",
    "    inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "    model = model_FC(inputs)\n",
    "    \n",
    "    # define callbacks (actions to be taken after each epoch of training)\n",
    "    file_lbl = \"{epoch:02d}-{loss:.4f}\"\n",
    "    filepath=\"weights-{0}.h5\".format(file_lbl)\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    tboard = callbacks.TensorBoard(log_dir='./logs', write_graph=True, write_images=False)\n",
    "    lcallbacks = [checkpoint, tboard]  \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "hist = model.fit(x_train, y_train, shuffle=True, epochs=60, batch_size=100, verbose=1, validation_split=0.05, callbacks=lcallbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "y_pred = model.predict(x_test, batch_size=100, verbose=0)\n",
    "y_bc = predict_barycenter(x_test)\n",
    "\n",
    "#for yt,yp,yb in zip(y_test,y_pred,y_bc):\n",
    "#    print(\"true = ({0},{1}); pred = ({2},{3}); barycenter = ({4},{5})\".format(fscale*(yt[0]-fshift),fscale*(yt[1]-fshift),fscale*(yp[0]-fshift),fscale*(yp[1]-fshift),yb[0],yb[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot (error in coordinate) vs. coordinate\n",
    "xtrue = fscale*(y_test[:,0] - fshift); ytrue = fscale*(y_test[:,1] - fshift)\n",
    "xpred = fscale*(y_pred[:,0] - fshift); ypred = fscale*(y_pred[:,1] - fshift)\n",
    "xbc = y_bc[:,0]; ybc = y_bc[:,1]\n",
    "xweights_pred = np.abs(xpred - xtrue); yweights_pred = np.abs(ypred - ytrue)\n",
    "xweights_bc = np.abs(xbc - xtrue); yweights_bc = np.abs(ybc - ytrue)\n",
    "\n",
    "# (average error in x) vs. x\n",
    "xcounts, xc_edges = np.histogram(xtrue,bins=50)\n",
    "xhist_pred, xbin_edges_pred = np.histogram(xtrue,weights=xweights_pred,bins=50)\n",
    "xbin_width_pred = xbin_edges_pred[1] - xbin_edges_pred[0]; xbin_centers_pred = xbin_edges_pred[:-1] + xbin_width_pred/2.\n",
    "xhist_pred /= xcounts\n",
    "\n",
    "xhist_bc, xbin_edges_bc = np.histogram(xtrue,weights=xweights_bc,bins=50)\n",
    "xbin_width_bc = xbin_edges_bc[1] - xbin_edges_bc[0]; xbin_centers_bc = xbin_edges_bc[:-1] + xbin_width_bc/2.\n",
    "xhist_bc /= xcounts\n",
    "\n",
    "# (average error in y) vs. y\n",
    "ycounts, yc_edges = np.histogram(ytrue,bins=50)\n",
    "yhist_pred, ybin_edges_pred = np.histogram(ytrue,weights=yweights_pred,bins=50)\n",
    "ybin_width_pred = ybin_edges_pred[1] - ybin_edges_pred[0]\n",
    "ybin_centers_pred = ybin_edges_pred[:-1] + ybin_width_pred/2.\n",
    "yhist_pred /= ycounts\n",
    "\n",
    "yhist_bc, ybin_edges_bc = np.histogram(ytrue,weights=yweights_bc,bins=50)\n",
    "ybin_width_bc = ybin_edges_bc[1] - ybin_edges_bc[0]\n",
    "ybin_centers_bc = ybin_edges_bc[:-1] + ybin_width_bc/2.\n",
    "yhist_bc /= ycounts\n",
    "\n",
    "# (average error) vs. r\n",
    "rvals = np.sqrt(xtrue**2 + ytrue**2)\n",
    "aevals_pred = np.sqrt(xweights_pred**2 + yweights_pred**2)\n",
    "aevals_bc = np.sqrt(xweights_bc**2 + yweights_bc**2)\n",
    "\n",
    "rcounts, rc_edges = np.histogram(rvals,bins=50)\n",
    "rhist_pred, rbin_edges_pred = np.histogram(rvals,weights=aevals_pred,bins=50)\n",
    "rbin_width_pred = rbin_edges_pred[1] - rbin_edges_pred[0]\n",
    "rbin_centers_pred = rbin_edges_pred[:-1] + rbin_width_pred/2.\n",
    "rhist_pred /= rcounts\n",
    "\n",
    "rhist_bc, rbin_edges_bc = np.histogram(rvals,weights=aevals_bc,bins=50)\n",
    "rbin_width_bc = rbin_edges_bc[1] - rbin_edges_bc[0]\n",
    "rbin_centers_bc = rbin_edges_bc[:-1] + rbin_width_bc/2.\n",
    "rhist_bc /= rcounts\n",
    "\n",
    "# create the plot\n",
    "fig = plt.figure();\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(15.0)\n",
    "\n",
    "ax1 = fig.add_subplot(131);\n",
    "ax1.axis([-200,200,0,max(max(xhist_pred),max(xhist_bc))])\n",
    "ax1.plot(xbin_centers_pred,xhist_pred,label='NN')\n",
    "ax1.plot(xbin_centers_bc,xhist_bc,label='barycenter')\n",
    "plt.legend(loc=2)\n",
    "ax1.set_xlabel(\"x (mm)\")\n",
    "ax1.set_ylabel(\"avg. error (mm)\")\n",
    "\n",
    "ax2 = fig.add_subplot(132);\n",
    "ax2.axis([-200,200,0,max(max(yhist_pred),max(yhist_bc))])\n",
    "ax2.plot(ybin_centers_pred,yhist_pred,label='NN')\n",
    "ax2.plot(ybin_centers_bc,yhist_bc,label='barycenter')\n",
    "plt.legend(loc=2)\n",
    "ax2.set_xlabel(\"y (mm)\")\n",
    "ax2.set_ylabel(\"avg. error (mm)\")\n",
    "\n",
    "ax3 = fig.add_subplot(133);\n",
    "ax3.axis([0,max(rvals),0,max(max(rhist_pred),max(rhist_bc))])\n",
    "ax3.plot(rbin_centers_pred,rhist_pred,label='NN')\n",
    "ax3.plot(rbin_centers_bc,rhist_bc,label='barycenter')\n",
    "plt.legend(loc=2)\n",
    "ax3.set_xlabel(\"r (mm)\")\n",
    "ax3.set_ylabel(\"avg. error (mm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
