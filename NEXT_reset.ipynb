{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classification with RESET-reconstructed events\n",
    "\n",
    "In this notebook we read in the prepared data, construct and train the DNN, and then evaluate its performance for the classification of RESET-reconstructed events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  as np\n",
    "import random as rd\n",
    "import tables as tb\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib.patches         import Ellipse\n",
    "from __future__  import print_function\n",
    "from scipy.stats import threshold\n",
    "\n",
    "# Keras imports\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.models               import Model, load_model, Sequential\n",
    "from keras.layers               import Input, Dense, MaxPooling3D, AveragePooling3D, Convolution3D, Activation, Dropout, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers           import SGD, Adam, Nadam         \n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.core          import Flatten\n",
    "from keras                      import callbacks\n",
    "from keras.regularizers         import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Variable definitions\n",
    "Here we define key variables to be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "xdim = 40\n",
    "ydim = 40\n",
    "vsizeX = 10.\n",
    "vsizeY = 10.\n",
    "\n",
    "# number of train/validation/test events per event type\n",
    "ntot = 110000\n",
    "ntest = 5000\n",
    "nval = 5000\n",
    "\n",
    "# data location and training/test file numbers\n",
    "data_fname_si = \"/home/jrenner/data/reset/reset_bb_V10_iter500.h5\"\n",
    "data_fname_bg = \"/home/jrenner/data/reset/reset_se_V10_iter500.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Function definitions\n",
    "\n",
    "### Data input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the function to read the data from multiple files\n",
    "def read_data(dataf_si, dataf_bg):\n",
    "    \"\"\"Reads all events from the files with the specified file numbers.\"\"\"\n",
    "    \n",
    "    # get the maps and the labels from the data files\n",
    "    indata_si = tb.open_file(dataf_si, 'r')\n",
    "    maps_si = indata_si.root.maps[0:ntot]\n",
    "    maps_si = np.reshape(maps_si, (len(maps_si), xdim, ydim))\n",
    "    labels_si = np.array(indata_si.root.coords[0:ntot],dtype=np.float32)\n",
    "    indata_si.close()\n",
    "    print(\"Read {0} signal events\".format(len(maps_si)))\n",
    "    \n",
    "    indata_bg = tb.open_file(dataf_bg, 'r')\n",
    "    maps_bg = indata_bg.root.maps[0:ntot]\n",
    "    maps_bg = np.reshape(maps_bg, (len(maps_bg), xdim, ydim))\n",
    "    labels_bg = np.array(indata_bg.root.coords[0:ntot],dtype=np.float32)\n",
    "    indata_bg.close()\n",
    "    print(\"Read {0} background events\".format(len(maps_bg)))\n",
    "    \n",
    "    # concatenate the datasets for training and test data\n",
    "    x_ = np.concatenate([maps_si[0:-ntest-nval], maps_bg[0:-ntest-nval]])\n",
    "    y_ = np.concatenate([labels_si[0:-ntest-nval], labels_bg[0:-ntest-nval]])\n",
    "    xval_ = np.concatenate([maps_si[-ntest-nval:-ntest], maps_bg[-ntest-nval:-ntest]])\n",
    "    yval_ = np.concatenate([labels_si[-ntest-nval:-ntest], labels_bg[-ntest-nval:-ntest]])\n",
    "    xtest_ = np.concatenate([maps_si[-ntest:], maps_bg[-ntest:]])\n",
    "    ytest_ = np.concatenate([labels_si[-ntest:], labels_bg[-ntest:]])\n",
    "    \n",
    "    # reshape the maps to add the extra channel dimension and the labels to fit in the interval [0,1]\n",
    "    x_ = np.reshape(x_, (len(x_), xdim, ydim, 1))\n",
    "    xval_ = np.reshape(xval_, (len(xval_), xdim, ydim, 1))\n",
    "    xtest_ = np.reshape(xtest_, (len(xtest_), xdim, ydim, 1))\n",
    "    \n",
    "    print(\"Finished reading data: {0} training events, {1} validation events, and {2} test events\".format(len(x_),len(xval_),len(xtest_)))\n",
    "    return x_,y_,xval_,yval_,xtest_,ytest_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural network models\n",
    "These functions should define and return a Keras model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a fully-connected neural network with 64 hidden neurons and 1 readout neuron\n",
    "def model_FC(inputs):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(xdim,ydim,1)))\n",
    "    model.add(Dense(output_dim=1024,activation='relu')) \n",
    "    model.add(Dense(output_dim=512,  activation='relu'))\n",
    "    model.add(Dense(output_dim=256,  activation='relu'))\n",
    "    model.add(Dense(output_dim=128, activation='relu'))\n",
    "    #model.add(Dense(output_dim=64, activation='relu'))\n",
    "    model.add(Dense(output_dim=1,    activation='relu'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Nadam(lr=0.00005, beta_1=0.9, beta_2=0.999,\n",
    "                                  epsilon=1e-08, schedule_decay=0.01),\n",
    "                                  metrics=['accuracy']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_CNN(inputs):\n",
    "#, kernel_regularizer=l2(0.1)    \n",
    "    cinputs = Conv2D(32, (5, 5), padding='same', strides=(2, 2), activation='relu', kernel_initializer='lecun_uniform')(inputs)\n",
    "    cinputs = AveragePooling2D(pool_size=(2, 2), data_format=None, padding=\"same\", strides=(2, 2))(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=3, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = Conv2D(128, (3, 3), padding='same', strides=(1, 1), activation='relu', kernel_initializer='lecun_uniform')(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=3, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = AveragePooling2D(pool_size=(2, 2), data_format=None, padding=\"same\", strides=(2, 2))(cinputs)\n",
    "    cinputs = Conv2D(256, (2, 2), padding='same', strides=(1, 1), activation='relu', kernel_initializer='lecun_uniform')(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=3, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = AveragePooling2D(pool_size=(2, 2), data_format=None, padding=\"same\", strides=(2, 2))(cinputs)\n",
    "    f1 = Flatten()(cinputs)\n",
    "    f1 = Dense(units=64, activation='relu', kernel_initializer='lecun_uniform')(f1)\n",
    "    f1 = Dropout(.4)(f1)\n",
    "\n",
    "    inc_output = Dense(units=1, activation='sigmoid', kernel_initializer='lecun_uniform')(f1)\n",
    "    model = Model(inputs, inc_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.00002, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.001), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot three RESET events\n",
    "# -- carried over from NEW_kr_diff_mc_train.ipynb\n",
    "def RESET_maps_plot(x_data, nstart, normalize=True):\n",
    "    \"\"\"\n",
    "    Plots a RESET voxel map\n",
    "    xarr is the RESET voxel map to plot\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_figheight(4.0)\n",
    "    fig.set_figwidth(17.5)\n",
    "    \n",
    "    for offset in range(3):\n",
    "    \n",
    "        xarr = x_data[nstart+offset,:,:,0]\n",
    "        \n",
    "        if normalize:\n",
    "            probs = (xarr - np.min(xarr))\n",
    "            probs /= np.max(probs)\n",
    "        else: \n",
    "            probs = xarr\n",
    "\n",
    "        ax = fig.add_subplot(int(\"13{0}\".format(offset+1)))\n",
    "        sp1 = plt.imshow(probs, extent=[-195., 195., -195., 195.], interpolation='none', aspect='auto', origin='lower')\n",
    "        plt.xlabel(\"x (mm)\")\n",
    "        plt.ylabel(\"y (mm)\")\n",
    "        cbp1 = plt.colorbar(sp1);\n",
    "        cbp1.set_label('Normalized Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = read_data(data_fname_si, data_fname_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot one slice of one event with corresponding true point\n",
    "RESET_maps_plot(x_train,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define and train the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set ld_model to true and specify the file to load in a previously defined/trained model\n",
    "load_weights = True\n",
    "mfile = 'models/weights_reset_V10_iter500.h5'\n",
    "\n",
    "if(load_weights):\n",
    "    model = load_model(mfile)\n",
    "else:\n",
    "    \n",
    "    # otherwise define the model\n",
    "    inputs = Input(shape=(xdim, ydim, 1))\n",
    "    model = model_CNN(inputs)\n",
    "    \n",
    "    # define callbacks (actions to be taken after each epoch of training)\n",
    "    file_lbl = \"{epoch:02d}-{loss:.4f}\"\n",
    "    filepath=\"weights-{0}.h5\".format(file_lbl)\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "    tboard = callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    lcallbacks = [checkpoint, tboard]            \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "hist = model.fit(x_train, y_train, shuffle=True, epochs=200, batch_size=100, verbose=1, validation_data=(x_val,y_val), callbacks=lcallbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compute the predictions\n",
    "loss_and_metrics = model.evaluate(x_test, y_test);\n",
    "y_pred = model.predict(x_test, batch_size=100, verbose=0)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create lists of values for signal vs. background curve\n",
    "npoints = 100\n",
    "fname_svsb = \"plt/reset_V10_iter500.h5\"\n",
    "bg_rej = []; si_eff = []\n",
    "print(\"-- Calculating points...\")\n",
    "for thh in np.arange(0,1,1./npoints):\n",
    "    nts = 0; ntb = 0\n",
    "    ncs = 0; ncb = 0\n",
    "    for ye,yp in zip(y_test,y_pred):\n",
    "        if(ye == 0):\n",
    "            ntb += 1  # add one background event\n",
    "            if(yp < thh):\n",
    "                ncb += 1  # add one correctly predicted background event\n",
    "\n",
    "        if(ye == 1):\n",
    "            nts += 1  # add one signal event\n",
    "            if(yp >= thh):\n",
    "                ncs += 1  # add one correctly predicted signal event\n",
    "                \n",
    "    si_eff.append(1.0*ncs/nts)\n",
    "    bg_rej.append(1.0*ncb/ntb)\n",
    "    #print(\"-- {0} of {1} ({2}%) correct background events; {3} of {4} ({5}%) correct signal events\".format(ncb,ntb,1.0*ncb/ntb*100,ncs,nts,1.0*ncs/nts*100))\n",
    "\n",
    "# save the results to file\n",
    "print(\"-- Saving results...\")\n",
    "si_eff = np.array(si_eff); bg_rej = np.array(bg_rej)\n",
    "f = tb.open_file(fname_svsb, 'w')\n",
    "filters = tb.Filters(complib='blosc', complevel=9, shuffle=False)\n",
    "\n",
    "atom    = tb.Atom.from_dtype(si_eff.dtype)\n",
    "sarr    = f.create_earray(f.root, 'si_eff', atom, (0, npoints), filters=filters)\n",
    "sarr.append([si_eff])\n",
    "\n",
    "atom    = tb.Atom.from_dtype(bg_rej.dtype)\n",
    "sarr    = f.create_earray(f.root, 'bg_rej', atom, (0, npoints), filters=filters)\n",
    "sarr.append([bg_rej])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot signal vs. background curves\n",
    "fnames = [\"plt/reset_V10_iter100.h5\", \"plt/reset_V10_iter500.h5\", \"plt/classification_V10_3Dconv.h5\"]\n",
    "labels = [\"RESET V10, iter100\", \"RESET V10, iter500\", \"SiPMs only (3D), V10\"]\n",
    "colors = [\"green\", \"black\", \"blue\"]\n",
    "\n",
    "# set up the plot\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(7.5)\n",
    "\n",
    "for nm,lb,co in zip(fnames,labels,colors):\n",
    "    \n",
    "    # read in the signal efficiency vs. background rejection information\n",
    "    fn = tb.open_file(nm,'r')\n",
    "    eff = fn.root.si_eff[0]\n",
    "    bgr = fn.root.bg_rej[0]\n",
    "    \n",
    "    plt.plot(eff,bgr,color=co,label=lb,lw=2)\n",
    "    fn.close()\n",
    "    \n",
    "plt.xlabel(\"signal efficiency\")\n",
    "plt.ylabel(\"background rejection\")\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
